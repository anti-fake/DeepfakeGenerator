import argparse
import json
import os
from typing import List, Tuple, Optional

import numpy as np
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
from scipy import linalg
from sklearn.cluster import KMeans
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm


def list_paths_recursive(root_dirs: List[str]) -> List[str]:
    """ì¬ê·€ì ìœ¼ë¡œ ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘"""
    paths = []
    for root_dir in root_dirs:
        if os.path.isdir(root_dir):
            for file in os.listdir(root_dir):
                path = os.path.join(root_dir, file)
                paths.extend(list_paths_recursive([path]))
        else:
            if root_dir.endswith(('.png', '.jpg', '.jpeg')):
                paths.append(root_dir)
    return paths


class ImageDataset(Dataset):
    def __init__(self, root_dirs: List[str]) -> None:
        super().__init__()
        self.root_dirs = root_dirs
        self.image_paths = list_paths_recursive(root_dirs)
        assert len(self.image_paths) > 0, "No images found"
        self.image_paths.sort()
        self.transform = transforms.Compose([
            transforms.Resize((299, 299), antialias=True),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

    def __len__(self) -> int:
        return len(self.image_paths)
    
    def __getitem__(self, idx: int) -> torch.Tensor:
        image_path = self.image_paths[idx]
        image = Image.open(image_path).convert("RGB")
        image = self.transform(image)
        return image


class InceptionV3FeatureExtractor(nn.Module):
    """InceptionV3ì˜ pool3 layerì—ì„œ 2048ì°¨ì› feature ì¶”ì¶œ"""
    def __init__(self):
        super().__init__()
        inception = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)
        self.blocks = nn.Sequential(
            inception.Conv2d_1a_3x3,
            inception.Conv2d_2a_3x3,
            inception.Conv2d_2b_3x3,
            nn.MaxPool2d(kernel_size=3, stride=2),
            inception.Conv2d_3b_1x1,
            inception.Conv2d_4a_3x3,
            nn.MaxPool2d(kernel_size=3, stride=2),
            inception.Mixed_5b,
            inception.Mixed_5c,
            inception.Mixed_5d,
            inception.Mixed_6a,
            inception.Mixed_6b,
            inception.Mixed_6c,
            inception.Mixed_6d,
            inception.Mixed_6e,
            inception.Mixed_7a,
            inception.Mixed_7b,
            inception.Mixed_7c,
            nn.AdaptiveAvgPool2d(output_size=(1, 1)),
        )
        
    def forward(self, x):
        x = self.blocks(x)
        return x.view(x.size(0), -1)


def extract_features(dataloader, model, device):
    """DataLoaderë¡œë¶€í„° feature ì¶”ì¶œ"""
    model.eval()
    all_features = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Extracting features"):
            batch = batch.to(device)
            features = model(batch)
            all_features.append(features.cpu().numpy())
    
    return np.concatenate(all_features, axis=0)


def compute_fid(features_A: np.ndarray, features_B: np.ndarray) -> Tuple[float, float, float]:
    """ë‘ feature ì„¸íŠ¸ ê°„ì˜ FID ê³„ì‚°"""
    mu_A = np.mean(features_A, axis=0)
    mu_B = np.mean(features_B, axis=0)
    sigma_A = np.cov(features_A, rowvar=False)
    sigma_B = np.cov(features_B, rowvar=False)
    
    diff = mu_A - mu_B
    mean_term = np.sum(diff ** 2)
    
    covmean = linalg.sqrtm(sigma_A @ sigma_B)
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    cov_term = np.trace(sigma_A + sigma_B - 2 * covmean)
    
    fid = mean_term + cov_term
    return fid, mean_term, cov_term


def compute_mahalanobis_distances(features_A: np.ndarray, features_B: np.ndarray) -> np.ndarray:
    """Bì˜ ê° ìƒ˜í”Œì—ì„œ A ë¶„í¬ê¹Œì§€ì˜ Mahalanobis ê±°ë¦¬ ê³„ì‚°"""
    mu_A = np.mean(features_A, axis=0)
    sigma_A = np.cov(features_A, rowvar=False)
    
    # ì •ê·œí™”ëœ ì—­í–‰ë ¬
    sigma_A_reg = sigma_A + np.eye(sigma_A.shape[0]) * 1e-6
    try:
        sigma_A_inv = np.linalg.inv(sigma_A_reg)
    except:
        sigma_A_inv = np.linalg.pinv(sigma_A_reg)
    
    diff = features_B - mu_A
    mahal_distances = np.sqrt(np.sum(diff @ sigma_A_inv * diff, axis=1))
    
    return mahal_distances


# ============================================================
# ğŸ¯ ìƒˆë¡œìš´ FID ê°œì„  ì „ëµë“¤ (v3)
# ============================================================

def strategy_aggressive_outlier_search(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    percentiles: List[int] = None
) -> Tuple[float, float, float, np.ndarray, dict]:
    """
    ì „ëµ 1: ê³µê²©ì  Outlier ì œê±° íƒìƒ‰
    percentileì„ 95â†’50ê¹Œì§€ ë‚´ë ¤ê°€ë©° FID ë³€í™” ì¶”ì 
    FIDê°€ ì¦ê°€í•˜ê¸° ì‹œì‘í•˜ëŠ” ìµœì ì  ìë™ íƒì§€
    """
    if percentiles is None:
        percentiles = list(range(95, 45, -5))  # 95, 90, 85, ..., 50
    
    print(f"\n[ì „ëµ 1] ê³µê²©ì  Outlier ì œê±° íƒìƒ‰")
    print(f"  íƒìƒ‰ ë²”ìœ„: {percentiles}")
    print("-" * 70)
    
    # Mahalanobis ê±°ë¦¬ ê³„ì‚° (í•œ ë²ˆë§Œ)
    mahal_distances = compute_mahalanobis_distances(features_A, features_B)
    
    results = {}
    best_fid = float('inf')
    best_percentile = 100
    best_indices = np.arange(len(features_B))
    
    # ì›ë³¸ FID
    orig_fid, orig_mean, orig_cov = compute_fid(features_A, features_B)
    results[100] = {'fid': orig_fid, 'mean': orig_mean, 'cov': orig_cov, 'n_samples': len(features_B)}
    print(f"  p=100: FID={orig_fid:.4f} (n={len(features_B)})")
    
    prev_fid = orig_fid
    
    for p in percentiles:
        threshold = np.percentile(mahal_distances, p)
        selected_indices = np.where(mahal_distances <= threshold)[0]
        
        if len(selected_indices) < 1000:  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ë³´ì¥
            print(f"  p={p}: ìƒ˜í”Œ ìˆ˜ ë¶€ì¡± ({len(selected_indices)}), ìŠ¤í‚µ")
            continue
        
        fid, mean_t, cov_t = compute_fid(features_A, features_B[selected_indices])
        results[p] = {'fid': fid, 'mean': mean_t, 'cov': cov_t, 'n_samples': len(selected_indices)}
        
        delta = fid - prev_fid
        marker = "â¬‡ï¸" if delta < 0 else "â¬†ï¸" if delta > 0 else "â¡ï¸"
        print(f"  p={p:2d}: FID={fid:.4f} (n={len(selected_indices):5d}) {marker} Î”={delta:+.4f}")
        
        if fid < best_fid:
            best_fid = fid
            best_percentile = p
            best_indices = selected_indices.copy()
        
        prev_fid = fid
        
        # ì¡°ê¸° ì¢…ë£Œ: FIDê°€ 3íšŒ ì—°ì† ì¦ê°€í•˜ë©´
        if len(results) >= 4:
            recent_fids = [results[k]['fid'] for k in sorted(results.keys(), reverse=True)[:4]]
            if all(recent_fids[i] <= recent_fids[i+1] for i in range(3)):
                print(f"  [ì¡°ê¸°ì¢…ë£Œ] FID ì—°ì† ì¦ê°€ ê°ì§€")
                break
    
    # ìµœì ì  ì¶œë ¥
    best_result = results[best_percentile]
    print("-" * 70)
    print(f"  ğŸ† ìµœì : p={best_percentile}, FID={best_fid:.4f}")
    print(f"     (í‰ê· : {best_result['mean']:.4f}, ê³µë¶„ì‚°: {best_result['cov']:.4f})")
    
    return best_fid, best_result['mean'], best_result['cov'], best_indices, results


def strategy_two_stage_hybrid(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    outlier_percentile: int = 85,
    target_size: Optional[int] = None,
    n_clusters: int = 50
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ 2: 2ë‹¨ê³„ ë³µí•© ì „ëµ
    Stage 1: Mahalanobis outlier ì œê±°
    Stage 2: KMeans stratified sampling (A í´ëŸ¬ìŠ¤í„° ë¹„ìœ¨ ìœ ì§€, ì¤‘ì‹¬ì— ê°€ê¹Œìš´ ìƒ˜í”Œ ìš°ì„ )
    """
    if target_size is None:
        target_size = len(features_A)
    
    print(f"\n[ì „ëµ 2] 2ë‹¨ê³„ ë³µí•© (outlier={outlier_percentile}%, target={target_size})")
    
    # Stage 1: Outlier ì œê±°
    mahal_distances = compute_mahalanobis_distances(features_A, features_B)
    threshold = np.percentile(mahal_distances, outlier_percentile)
    stage1_mask = mahal_distances <= threshold
    stage1_indices = np.where(stage1_mask)[0]
    stage1_features = features_B[stage1_indices]
    
    print(f"  Stage 1 í›„: {len(stage1_indices)}ê°œ")
    
    # Stage 2: Stratified Sampling
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    kmeans.fit(features_A)
    
    labels_A = kmeans.labels_
    labels_B = kmeans.predict(stage1_features)
    
    # Aì˜ í´ëŸ¬ìŠ¤í„°ë³„ ë¹„ìœ¨
    cluster_counts_A = np.bincount(labels_A, minlength=n_clusters)
    cluster_ratios = cluster_counts_A / len(features_A)
    
    selected_local_indices = []
    
    for cluster_id in range(n_clusters):
        n_needed = max(1, int(cluster_ratios[cluster_id] * target_size))
        cluster_local_indices = np.where(labels_B == cluster_id)[0]
        
        if len(cluster_local_indices) == 0:
            continue
        
        # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì— ê°€ê¹Œìš´ ìˆœìœ¼ë¡œ ì •ë ¬
        center = kmeans.cluster_centers_[cluster_id]
        cluster_features = stage1_features[cluster_local_indices]
        distances = np.linalg.norm(cluster_features - center, axis=1)
        sorted_local = cluster_local_indices[np.argsort(distances)]
        
        n_select = min(n_needed, len(sorted_local))
        selected_local_indices.extend(sorted_local[:n_select].tolist())
    
    # ì›ë˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
    selected_indices = stage1_indices[selected_local_indices]
    print(f"  Stage 2 í›„: {len(selected_indices)}ê°œ")
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[selected_indices])
    print(f"  â†’ FID: {fid:.4f} (í‰ê· : {mean_t:.4f}, ê³µë¶„ì‚°: {cov_t:.4f})")
    
    return fid, mean_t, cov_t, selected_indices


def strategy_iterative_removal(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    target_fid: float = 20.0,
    max_remove_ratio: float = 0.5,
    removal_rate: float = 0.01,
    max_iter: int = 100
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ 3: Iterative Refinement
    ë§¤ iterationë§ˆë‹¤ FID ê¸°ì—¬ë„ê°€ ë†’ì€ ìƒìœ„ ìƒ˜í”Œ ì œê±°
    ì¢…ë£Œ ì¡°ê±´: FIDê°€ ëª©í‘œ ë„ë‹¬ ë˜ëŠ” ì—°ì† ì¦ê°€
    """
    print(f"\n[ì „ëµ 3] Iterative Removal (ëª©í‘œ: {target_fid}, ìµœëŒ€ ì œê±°: {max_remove_ratio*100}%)")
    
    current_indices = np.arange(len(features_B))
    min_samples = int(len(features_B) * (1 - max_remove_ratio))
    
    current_fid, current_mean, current_cov = compute_fid(features_A, features_B)
    best_fid = current_fid
    best_indices = current_indices.copy()
    
    print(f"  ì´ˆê¸°: FID={current_fid:.4f} (n={len(current_indices)})")
    
    fid_history = [current_fid]
    consecutive_increase = 0
    
    for iteration in range(max_iter):
        if len(current_indices) <= min_samples:
            print(f"  [ì¢…ë£Œ] ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ë„ë‹¬")
            break
        
        if current_fid <= target_fid:
            print(f"  [ì¢…ë£Œ] ëª©í‘œ FID ë„ë‹¬!")
            break
        
        # í˜„ì¬ ìƒ˜í”Œë“¤ì˜ FID ê¸°ì—¬ë„ ê³„ì‚° (leave-one-out ê·¼ì‚¬)
        current_features = features_B[current_indices]
        mu_A = np.mean(features_A, axis=0)
        mu_B = np.mean(current_features, axis=0)
        
        # í‰ê· ì— ëŒ€í•œ ê¸°ì—¬ë„: ê° ìƒ˜í”Œì´ í‰ê· ì„ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚˜ê²Œ í•˜ëŠ”ê°€
        mean_contribution = np.sum((current_features - mu_A) ** 2, axis=1)
        
        # Mahalanobis ê±°ë¦¬ (ê³µë¶„ì‚° ê¸°ì—¬ë„ ê·¼ì‚¬)
        mahal_dist = compute_mahalanobis_distances(features_A, current_features)
        
        # ì¢…í•© ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ë‚˜ì¨)
        combined_score = mean_contribution + mahal_dist ** 2
        
        # ìƒìœ„ removal_rate ë¹„ìœ¨ ì œê±°
        n_remove = max(1, int(len(current_indices) * removal_rate))
        remove_local_indices = np.argsort(combined_score)[-n_remove:]
        
        # ì œê±°
        keep_mask = np.ones(len(current_indices), dtype=bool)
        keep_mask[remove_local_indices] = False
        current_indices = current_indices[keep_mask]
        
        # ìƒˆ FID ê³„ì‚°
        new_fid, new_mean, new_cov = compute_fid(features_A, features_B[current_indices])
        
        delta = new_fid - current_fid
        if (iteration + 1) % 10 == 0 or delta < 0:
            marker = "â¬‡ï¸" if delta < 0 else "â¬†ï¸"
            print(f"  Iter {iteration+1:3d}: FID={new_fid:.4f} (n={len(current_indices):5d}) {marker} Î”={delta:+.4f}")
        
        if new_fid < best_fid:
            best_fid = new_fid
            best_indices = current_indices.copy()
            consecutive_increase = 0
        else:
            consecutive_increase += 1
        
        if consecutive_increase >= 5:
            print(f"  [ì¢…ë£Œ] FID ì—°ì† ì¦ê°€")
            break
        
        current_fid = new_fid
        fid_history.append(current_fid)
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[best_indices])
    print(f"  â†’ ìµœì¢… FID: {fid:.4f} (í‰ê· : {mean_t:.4f}, ê³µë¶„ì‚°: {cov_t:.4f})")
    print(f"  â†’ ì„ íƒëœ ìƒ˜í”Œ: {len(best_indices)}ê°œ")
    
    return fid, mean_t, cov_t, best_indices


def strategy_covariance_greedy(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    target_size: Optional[int] = None,
    n_iter: int = 500,
    batch_size: int = 100
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ 4: ê³µë¶„ì‚° ì§ì ‘ ë§¤ì¹­ (Greedy)
    Aì˜ ê³µë¶„ì‚° í–‰ë ¬ì„ targetìœ¼ë¡œ
    B subsetì˜ ê³µë¶„ì‚°ì´ Aì™€ ìµœëŒ€í•œ ë¹„ìŠ·í•´ì§€ë„ë¡ ìƒ˜í”Œ ì„ íƒ
    ||sigma_A - sigma_B_subset||_F ìµœì†Œí™”
    """
    if target_size is None:
        target_size = len(features_A) * 2
    
    print(f"\n[ì „ëµ 4] ê³µë¶„ì‚° Greedy ë§¤ì¹­ (target={target_size})")
    
    sigma_A = np.cov(features_A, rowvar=False)
    mu_A = np.mean(features_A, axis=0)
    
    # ì´ˆê¸°í™”: Mahalanobis ê±°ë¦¬ ê¸°ì¤€ ìƒìœ„ ìƒ˜í”Œë“¤ë¡œ ì‹œì‘
    mahal_distances = compute_mahalanobis_distances(features_A, features_B)
    init_indices = np.argsort(mahal_distances)[:target_size]
    
    current_indices = list(init_indices)
    remaining_indices = list(set(range(len(features_B))) - set(current_indices))
    
    current_sigma = np.cov(features_B[current_indices], rowvar=False)
    current_frob = np.linalg.norm(sigma_A - current_sigma, 'fro')
    
    print(f"  ì´ˆê¸° Frobenius: {current_frob:.4f}")
    
    best_frob = current_frob
    best_indices = current_indices.copy()
    
    for iteration in tqdm(range(n_iter), desc="  Greedy ìµœì í™”"):
        improved = False
        
        # ëœë¤í•˜ê²Œ swap ì‹œë„
        np.random.seed(iteration)
        swap_candidates = np.random.choice(len(current_indices), size=min(batch_size, len(current_indices)), replace=False)
        
        for local_idx in swap_candidates:
            idx_out = current_indices[local_idx]
            
            # ëœë¤í•˜ê²Œ êµì²´ í›„ë³´ ì„ íƒ
            candidates_in = np.random.choice(remaining_indices, size=min(10, len(remaining_indices)), replace=False)
            
            for idx_in in candidates_in:
                test_indices = current_indices.copy()
                test_indices[local_idx] = idx_in
                
                test_sigma = np.cov(features_B[test_indices], rowvar=False)
                test_frob = np.linalg.norm(sigma_A - test_sigma, 'fro')
                
                if test_frob < current_frob:
                    remaining_indices.remove(idx_in)
                    remaining_indices.append(idx_out)
                    current_indices[local_idx] = idx_in
                    current_frob = test_frob
                    improved = True
                    
                    if current_frob < best_frob:
                        best_frob = current_frob
                        best_indices = current_indices.copy()
                    break
            
            if improved:
                break
        
        if (iteration + 1) % 100 == 0:
            tqdm.write(f"    Iter {iteration+1}: Frob={current_frob:.4f}")
    
    print(f"  ìµœì¢… Frobenius: {best_frob:.4f}")
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[best_indices])
    print(f"  â†’ FID: {fid:.4f} (í‰ê· : {mean_t:.4f}, ê³µë¶„ì‚°: {cov_t:.4f})")
    
    return fid, mean_t, cov_t, np.array(best_indices)


def strategy_sinkhorn_ot(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    target_size: Optional[int] = None,
    reg: float = 0.05,
    device: str = 'cuda'
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ 5: Sinkhorn OT ê¸°ë°˜ ì„ íƒ (GPU ê°€ì†)
    Optimal Transportë¡œ importance score ê³„ì‚°
    ìƒìœ„ target_sizeê°œ ì„ íƒ
    """
    if target_size is None:
        target_size = len(features_A) * 2
    
    print(f"\n[ì „ëµ 5] Sinkhorn OT (target={target_size}, reg={reg})")
    
    try:
        import ot
    except ImportError:
        print("  âš ï¸ POT ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”: pip install POT")
        return float('inf'), 0, 0, np.array([])
    
    n_A = len(features_A)
    n_B = len(features_B)
    
    # Uniform distributions
    a = np.ones(n_A) / n_A
    b = np.ones(n_B) / n_B
    
    print(f"  Cost matrix ê³„ì‚° ì¤‘... ({n_A} x {n_B})")
    
    # Cost matrix (L2 distance)
    # GPU ì‚¬ìš© ì‹œ
    if device == 'cuda' and torch.cuda.is_available():
        A_torch = torch.from_numpy(features_A).float().cuda()
        B_torch = torch.from_numpy(features_B).float().cuda()
        
        # Chunked computation to avoid OOM
        chunk_size = 10000
        cost_chunks = []
        
        for i in range(0, n_B, chunk_size):
            end_i = min(i + chunk_size, n_B)
            B_chunk = B_torch[i:end_i]
            
            # (n_A, chunk) distance matrix
            diff = A_torch.unsqueeze(1) - B_chunk.unsqueeze(0)
            cost_chunk = torch.sum(diff ** 2, dim=2).cpu().numpy()
            cost_chunks.append(cost_chunk)
        
        cost_matrix = np.concatenate(cost_chunks, axis=1)
        del A_torch, B_torch
        torch.cuda.empty_cache()
    else:
        # CPU fallback
        from scipy.spatial.distance import cdist
        cost_matrix = cdist(features_A, features_B, metric='sqeuclidean')
    
    print(f"  Sinkhorn ê³„ì‚° ì¤‘... (reg={reg})")
    
    # Sinkhorn (CPU, POT library)
    try:
        T = ot.sinkhorn(a, b, cost_matrix, reg=reg, numItermax=1000, stopThr=1e-9)
    except Exception as e:
        print(f"  âš ï¸ Sinkhorn ì‹¤íŒ¨: {e}")
        # Fallback to simpler OT
        T = ot.emd(a, b, cost_matrix)
    
    # Importance score: ê° B ìƒ˜í”Œì´ Aì— ì–¼ë§ˆë‚˜ ë§¤ì¹­ë˜ëŠ”ê°€
    importance = T.sum(axis=0)
    
    # ìƒìœ„ target_sizeê°œ ì„ íƒ
    selected_indices = np.argsort(importance)[-target_size:]
    
    print(f"  ì„ íƒëœ ìƒ˜í”Œ: {len(selected_indices)}ê°œ")
    print(f"  Importance ë¶„í¬: min={importance.min():.6f}, max={importance.max():.6f}")
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[selected_indices])
    print(f"  â†’ FID: {fid:.4f} (í‰ê· : {mean_t:.4f}, ê³µë¶„ì‚°: {cov_t:.4f})")
    
    return fid, mean_t, cov_t, selected_indices


def strategy_eigenvalue_matching(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    target_size: Optional[int] = None,
    n_components: int = 100
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ 6: ê³ ìœ ê°’ ë¶„í¬ ë§¤ì¹­
    Aì˜ ì£¼ìš” ì£¼ì„±ë¶„ë³„ ë¶„ì‚°ì— ë§ì¶° B ìƒ˜í”Œ ì„ íƒ
    """
    if target_size is None:
        target_size = len(features_A) * 2
    
    print(f"\n[ì „ëµ 6] ê³ ìœ ê°’ ë¶„í¬ ë§¤ì¹­ (target={target_size})")
    
    from sklearn.decomposition import PCA
    
    # Aë¡œ PCA í•™ìŠµ
    pca = PCA(n_components=min(n_components, features_A.shape[1]))
    pca.fit(features_A)
    
    A_pca = pca.transform(features_A)
    B_pca = pca.transform(features_B)
    
    # Aì˜ ê° ì£¼ì„±ë¶„ë³„ ë¶„ì‚° ë° ë²”ìœ„
    var_A = np.var(A_pca, axis=0)
    mean_A_pca = np.mean(A_pca, axis=0)
    std_A_pca = np.std(A_pca, axis=0)
    
    # Bì˜ ê° ìƒ˜í”Œì´ Aì˜ ë¶„í¬ì™€ ì–¼ë§ˆë‚˜ ë§ëŠ”ì§€ ì ìˆ˜í™”
    # ì£¼ìš” PCë“¤ì—ì„œ Aì˜ ë¶„í¬ ë²”ìœ„ ë‚´ì— ìˆëŠ” ì •ë„
    scores = np.zeros(len(features_B))
    
    for pc_idx in range(min(20, len(var_A))):  # ìƒìœ„ 20ê°œ PCë§Œ ì‚¬ìš©
        # Aì˜ í•´ë‹¹ PC ë¶„í¬ (í‰ê·  Â± 2*std ë²”ìœ„)
        low = mean_A_pca[pc_idx] - 2 * std_A_pca[pc_idx]
        high = mean_A_pca[pc_idx] + 2 * std_A_pca[pc_idx]
        
        # Bì˜ í•´ë‹¹ PC ê°’ì´ ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©´ ê°€ì‚°ì 
        in_range = (B_pca[:, pc_idx] >= low) & (B_pca[:, pc_idx] <= high)
        scores += in_range.astype(float) * (var_A[pc_idx] / var_A.sum())  # ë¶„ì‚° ë¹„ìœ¨ë¡œ ê°€ì¤‘
        
        # ë²”ìœ„ì—ì„œ ë²—ì–´ë‚œ ì •ë„ì— ë”°ë¼ í˜ë„í‹°
        deviation = np.abs(B_pca[:, pc_idx] - mean_A_pca[pc_idx]) / (std_A_pca[pc_idx] + 1e-6)
        scores -= deviation * 0.01 * (var_A[pc_idx] / var_A.sum())
    
    # ìƒìœ„ target_sizeê°œ ì„ íƒ
    selected_indices = np.argsort(scores)[-target_size:]
    
    # ê²€ì¦: ì„ íƒëœ ìƒ˜í”Œì˜ ë¶„ì‚° ë¹„êµ
    selected_pca = B_pca[selected_indices]
    var_selected = np.var(selected_pca, axis=0)
    
    print(f"  ìƒìœ„ 5ê°œ PC ë¶„ì‚° ë¹„êµ:")
    for i in range(5):
        ratio = var_selected[i] / var_A[i] if var_A[i] > 0 else 0
        print(f"    PC{i+1}: A={var_A[i]:.4f}, Selected={var_selected[i]:.4f}, ratio={ratio:.2f}x")
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[selected_indices])
    print(f"  â†’ FID: {fid:.4f} (í‰ê· : {mean_t:.4f}, ê³µë¶„ì‚°: {cov_t:.4f})")
    
    return fid, mean_t, cov_t, selected_indices


def strategy_combined_best(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    device: str = 'cuda'
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ 7: ìµœì  ì¡°í•© íƒìƒ‰
    ì—¬ëŸ¬ ì „ëµì˜ ê²°ê³¼ë¥¼ ì¡°í•©í•˜ì—¬ ìµœì  ì„ íƒ
    """
    print(f"\n[ì „ëµ 7] ìµœì  ì¡°í•© íƒìƒ‰")
    
    results = []
    
    # ê° ì „ëµë³„ ìµœì  ê²°ê³¼ ìˆ˜ì§‘
    # 1. Aggressive outlier search
    try:
        fid1, mean1, cov1, indices1, _ = strategy_aggressive_outlier_search(
            features_A, features_B, 
            percentiles=[95, 90, 85, 80, 75]
        )
        results.append(("Aggressive Outlier", fid1, mean1, cov1, indices1))
    except Exception as e:
        print(f"  ì „ëµ1 ì‹¤íŒ¨: {e}")
    
    # 2. Two-stage hybrid (ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°)
    for outlier_p in [90, 85, 80]:
        for target_mult in [1, 2, 4]:
            try:
                target = len(features_A) * target_mult
                fid2, mean2, cov2, indices2 = strategy_two_stage_hybrid(
                    features_A, features_B, 
                    outlier_percentile=outlier_p,
                    target_size=target
                )
                results.append((f"Hybrid(p={outlier_p},t={target})", fid2, mean2, cov2, indices2))
            except Exception as e:
                pass
    
    # 3. Iterative removal
    try:
        fid3, mean3, cov3, indices3 = strategy_iterative_removal(
            features_A, features_B,
            target_fid=20.0,
            max_remove_ratio=0.3
        )
        results.append(("Iterative Removal", fid3, mean3, cov3, indices3))
    except Exception as e:
        print(f"  ì „ëµ3 ì‹¤íŒ¨: {e}")
    
    # ê²°ê³¼ ì •ë ¬
    results.sort(key=lambda x: x[1])
    
    print("\n  ì¡°í•© íƒìƒ‰ ê²°ê³¼:")
    print("-" * 70)
    for name, fid, mean_t, cov_t, _ in results[:10]:
        print(f"    {name:<35} FID={fid:.4f} (m={mean_t:.4f}, c={cov_t:.4f})")
    
    if results:
        best_name, best_fid, best_mean, best_cov, best_indices = results[0]
        print(f"\n  ğŸ† ìµœê³ : {best_name}, FID={best_fid:.4f}")
        return best_fid, best_mean, best_cov, best_indices
    else:
        return float('inf'), 0, 0, np.array([])


# ============================================================
# ğŸ¯ ìƒˆë¡œìš´ FID ê°œì„  ì „ëµë“¤ (v4) - ê³µë¶„ì‚° term 17 ì´í•˜ ëª©í‘œ
# ============================================================

def strategy_fine_iterative(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    removal_rate: float = 0.005,  # 0.5%
    patience: int = 10,
    max_remove_ratio: float = 0.5,
    target_cov_term: float = 17.0
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ v4-1: Fine-grained Iterative Removal
    - 0.5%ì”© ì„¸ë°€í•˜ê²Œ ì œê±°
    - patience=10ìœ¼ë¡œ early stopping
    - ê³µë¶„ì‚° term 17 ì´í•˜ ëª©í‘œ
    """
    print(f"\n[v4-1] Fine Iterative (rate={removal_rate*100}%, patience={patience})")
    
    current_indices = np.arange(len(features_B))
    min_samples = int(len(features_B) * (1 - max_remove_ratio))
    
    current_fid, current_mean, current_cov = compute_fid(features_A, features_B)
    best_fid = current_fid
    best_cov = current_cov
    best_indices = current_indices.copy()
    
    print(f"  ì´ˆê¸°: FID={current_fid:.4f} (í‰ê· ={current_mean:.4f}, ê³µë¶„ì‚°={current_cov:.4f})")
    
    no_improve_count = 0
    iteration = 0
    
    while len(current_indices) > min_samples:
        iteration += 1
        
        # í˜„ì¬ ìƒ˜í”Œë“¤ì˜ FID ê¸°ì—¬ë„ ê³„ì‚°
        current_features = features_B[current_indices]
        mu_A = np.mean(features_A, axis=0)
        
        # Mahalanobis ê±°ë¦¬ (ê³µë¶„ì‚° ê¸°ì—¬ë„ ê·¼ì‚¬)
        mahal_dist = compute_mahalanobis_distances(features_A, current_features)
        
        # í‰ê·  ê¸°ì—¬ë„
        mean_contribution = np.sum((current_features - mu_A) ** 2, axis=1)
        
        # ê³µë¶„ì‚° í¸í–¥ ì ìˆ˜ (ê³µë¶„ì‚° term ê°ì†Œì— ì§‘ì¤‘)
        combined_score = 0.3 * mean_contribution + 0.7 * (mahal_dist ** 2)
        
        # ìƒìœ„ removal_rate ë¹„ìœ¨ ì œê±°
        n_remove = max(1, int(len(current_indices) * removal_rate))
        remove_local_indices = np.argsort(combined_score)[-n_remove:]
        
        # ì œê±°
        keep_mask = np.ones(len(current_indices), dtype=bool)
        keep_mask[remove_local_indices] = False
        current_indices = current_indices[keep_mask]
        
        # ìƒˆ FID ê³„ì‚°
        new_fid, new_mean, new_cov = compute_fid(features_A, features_B[current_indices])
        
        # ê°œì„  ì²´í¬
        if new_fid < best_fid:
            best_fid = new_fid
            best_cov = new_cov
            best_indices = current_indices.copy()
            no_improve_count = 0
            
            if iteration % 20 == 0 or new_cov < target_cov_term:
                print(f"  Iter {iteration:4d}: FID={new_fid:.4f} (m={new_mean:.4f}, c={new_cov:.4f}) â¬‡ï¸ n={len(current_indices)}")
        else:
            no_improve_count += 1
        
        # ëª©í‘œ ë‹¬ì„±
        if new_cov <= target_cov_term:
            print(f"  ğŸ¯ ê³µë¶„ì‚° term ëª©í‘œ ë‹¬ì„±! c={new_cov:.4f}")
            break
        
        # Early stopping
        if no_improve_count >= patience:
            print(f"  [Early Stop] {patience}íšŒ ì—°ì† ë¯¸ê°œì„ ")
            break
        
        current_fid = new_fid
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[best_indices])
    print(f"  â†’ ìµœì¢…: FID={fid:.4f} (í‰ê· ={mean_t:.4f}, ê³µë¶„ì‚°={cov_t:.4f})")
    print(f"  â†’ ìƒ˜í”Œ ìˆ˜: {len(best_indices)} ({len(best_indices)/len(features_B)*100:.1f}%)")
    
    return fid, mean_t, cov_t, best_indices


def strategy_dimension_targeted(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    top_k_dims: int = 200,
    outlier_percentile: float = 90
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ v4-2: Dimension Targeted Outlier Removal
    - ë¶„ì‚° ì°¨ì´ê°€ í° ì°¨ì› ì‹ë³„
    - í•´ë‹¹ ì°¨ì›ì—ì„œë§Œ outlier ì œê±°
    - Aì˜ ë¶„ì‚° ë¶„í¬ì™€ ë§ì§€ ì•ŠëŠ” ìƒ˜í”Œ ì œê±°
    """
    print(f"\n[v4-2] Dimension Targeted (top_k={top_k_dims}, percentile={outlier_percentile})")
    
    # ì°¨ì›ë³„ ë¶„ì‚° ê³„ì‚°
    var_A = np.var(features_A, axis=0)
    var_B = np.var(features_B, axis=0)
    
    # ë¶„ì‚° ì°¨ì´ ë¹„ìœ¨
    var_ratio = var_B / (var_A + 1e-10)
    
    # ë¶„ì‚° ì°¨ì´ê°€ í° ì°¨ì› (ê³¼ëŒ€ ë˜ëŠ” ê³¼ì†Œ)
    var_diff = np.abs(var_ratio - 1.0)
    target_dims = np.argsort(var_diff)[-top_k_dims:]
    
    print(f"  íƒ€ê²Ÿ ì°¨ì› {top_k_dims}ê°œ ì„ íƒ")
    print(f"  ë¶„ì‚° ë¹„ìœ¨ ë²”ìœ„: {var_ratio[target_dims].min():.2f} ~ {var_ratio[target_dims].max():.2f}")
    
    # íƒ€ê²Ÿ ì°¨ì›ì—ì„œì˜ outlier ì ìˆ˜ ê³„ì‚°
    mu_A_target = np.mean(features_A[:, target_dims], axis=0)
    std_A_target = np.std(features_A[:, target_dims], axis=0) + 1e-10
    
    # Bì˜ ê° ìƒ˜í”Œì´ íƒ€ê²Ÿ ì°¨ì›ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚˜ëŠ”ì§€
    B_target = features_B[:, target_dims]
    z_scores = np.abs((B_target - mu_A_target) / std_A_target)
    outlier_scores = np.mean(z_scores, axis=1)
    
    # ìƒìœ„ outlier ì œê±°
    threshold = np.percentile(outlier_scores, outlier_percentile)
    selected_indices = np.where(outlier_scores <= threshold)[0]
    
    print(f"  ì„ íƒëœ ìƒ˜í”Œ: {len(selected_indices)} / {len(features_B)}")
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[selected_indices])
    print(f"  â†’ FID: {fid:.4f} (í‰ê· : {mean_t:.4f}, ê³µë¶„ì‚°: {cov_t:.4f})")
    
    return fid, mean_t, cov_t, selected_indices


def strategy_eigenspace_variance_match(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    n_components: int = 50,
    tolerance: float = 0.3
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ v4-3: Eigenspace Variance Matching
    - Aì˜ PCA ê³µê°„ì—ì„œ ê° PCë³„ ë¶„ì‚° íƒ€ê²Ÿ
    - Bì—ì„œ í•´ë‹¹ ë¶„ì‚°ì— ê¸°ì—¬í•˜ëŠ” ìƒ˜í”Œ ì„ íƒ
    - ë¶„ì‚° ë¹„ìœ¨ì´ tolerance ë²”ìœ„ ë‚´ì¸ ìƒ˜í”Œ ìš°ì„ 
    """
    print(f"\n[v4-3] Eigenspace Variance Match (n_comp={n_components}, tol={tolerance})")
    
    from sklearn.decomposition import PCA
    
    # Aë¡œ PCA í•™ìŠµ
    pca = PCA(n_components=min(n_components, features_A.shape[1], len(features_A) - 1))
    pca.fit(features_A)
    
    A_pca = pca.transform(features_A)
    B_pca = pca.transform(features_B)
    
    # Aì˜ ê° PCë³„ í†µê³„
    mean_A_pca = np.mean(A_pca, axis=0)
    std_A_pca = np.std(A_pca, axis=0)
    var_A_pca = np.var(A_pca, axis=0)
    
    # ê° B ìƒ˜í”Œì˜ "ë¶„ì‚° ê¸°ì—¬ë„" ì ìˆ˜
    # ê° PCì—ì„œ ë¶„ì‚°ì— ê¸°ì—¬í•˜ëŠ” ì •ë„ (ì œê³± í¸ì°¨)
    B_centered = B_pca - mean_A_pca
    
    # ì´ìƒì ì¸ ë¶„ì‚° ê¸°ì—¬ë„ (Aì™€ ë™ì¼í•œ ë¶„ì‚°ì„ ë§Œë“¤ê¸° ìœ„í•´)
    ideal_sq_dev = var_A_pca  # ê° PCì˜ ë¶„ì‚°
    
    # ê° ìƒ˜í”Œì˜ ì œê³± í¸ì°¨
    sample_sq_dev = B_centered ** 2
    
    # ë¶„ì‚° ë¹„ìœ¨ ì ìˆ˜: 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ
    # ê° PCë³„ë¡œ (sample_sq_dev / ideal_sq_dev) ê°€ 1ì— ê°€ê¹Œìš´ ì •ë„
    var_contribution = sample_sq_dev / (ideal_sq_dev + 1e-10)
    
    # ìƒìœ„ PCë“¤ì˜ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì ìˆ˜í™”
    weights = var_A_pca / var_A_pca.sum()  # ë¶„ì‚° ë¹„ìœ¨ì„ ê°€ì¤‘ì¹˜ë¡œ
    
    # 1ì—ì„œ ë²—ì–´ë‚œ ì •ë„ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
    deviation_from_one = np.abs(var_contribution - 1.0)
    weighted_deviation = np.sum(deviation_from_one * weights, axis=1)
    
    # tolerance ë²”ìœ„ ë‚´ ìƒ˜í”Œ ìš°ì„  ì„ íƒ
    good_samples = weighted_deviation < tolerance
    
    if good_samples.sum() < 1000:
        # ë„ˆë¬´ ì ìœ¼ë©´ ìƒìœ„ 50% ì„ íƒ
        threshold = np.percentile(weighted_deviation, 50)
        good_samples = weighted_deviation < threshold
    
    selected_indices = np.where(good_samples)[0]
    
    print(f"  ì„ íƒëœ ìƒ˜í”Œ: {len(selected_indices)} / {len(features_B)}")
    
    # ì„ íƒëœ ìƒ˜í”Œì˜ ë¶„ì‚° ë¹„êµ
    selected_pca = B_pca[selected_indices]
    var_selected = np.var(selected_pca, axis=0)
    
    print(f"  ìƒìœ„ 5ê°œ PC ë¶„ì‚° ë¹„êµ:")
    for i in range(min(5, len(var_A_pca))):
        ratio = var_selected[i] / var_A_pca[i] if var_A_pca[i] > 0 else 0
        print(f"    PC{i+1}: A={var_A_pca[i]:.4f}, Sel={var_selected[i]:.4f}, ratio={ratio:.3f}")
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[selected_indices])
    print(f"  â†’ FID: {fid:.4f} (í‰ê· : {mean_t:.4f}, ê³µë¶„ì‚°: {cov_t:.4f})")
    
    return fid, mean_t, cov_t, selected_indices


def strategy_minibatch_sinkhorn(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    target_size: Optional[int] = None,
    batch_size: int = 5000,
    n_iter: int = 30,
    reg: float = 0.1,
    device: str = 'cuda'
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ v4-4: Mini-batch Sinkhorn OT
    - OOM ë°©ì§€ë¥¼ ìœ„í•œ mini-batch OT
    - importance score ëˆ„ì  í›„ ìƒìœ„ ì„ íƒ
    """
    if target_size is None:
        target_size = len(features_A) * 4
    
    print(f"\n[v4-4] Minibatch Sinkhorn (batch={batch_size}, iter={n_iter}, reg={reg})")
    
    try:
        import ot
    except ImportError:
        print("  âš ï¸ POT ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”: pip install POT")
        return float('inf'), 0, 0, np.array([])
    
    n_A = len(features_A)
    n_B = len(features_B)
    
    # Importance score ëˆ„ì 
    importance_scores = np.zeros(n_B)
    
    for iteration in tqdm(range(n_iter), desc="  Minibatch OT"):
        # Aì—ì„œ ëœë¤ ìƒ˜í”Œë§
        np.random.seed(iteration)
        A_sample_idx = np.random.choice(n_A, size=min(batch_size, n_A), replace=False)
        B_sample_idx = np.random.choice(n_B, size=min(batch_size, n_B), replace=False)
        
        A_batch = features_A[A_sample_idx]
        B_batch = features_B[B_sample_idx]
        
        # Cost matrix
        if device == 'cuda' and torch.cuda.is_available():
            A_t = torch.from_numpy(A_batch).float().cuda()
            B_t = torch.from_numpy(B_batch).float().cuda()
            diff = A_t.unsqueeze(1) - B_t.unsqueeze(0)
            cost = torch.sum(diff ** 2, dim=2).cpu().numpy()
            del A_t, B_t
            torch.cuda.empty_cache()
        else:
            from scipy.spatial.distance import cdist
            cost = cdist(A_batch, B_batch, metric='sqeuclidean')
        
        # Uniform distributions
        a = np.ones(len(A_batch)) / len(A_batch)
        b = np.ones(len(B_batch)) / len(B_batch)
        
        # Sinkhorn
        try:
            T = ot.sinkhorn(a, b, cost, reg=reg, numItermax=500, stopThr=1e-6)
            batch_importance = T.sum(axis=0)
            importance_scores[B_sample_idx] += batch_importance
        except Exception as e:
            continue
    
    # ìƒìœ„ target_sizeê°œ ì„ íƒ
    selected_indices = np.argsort(importance_scores)[-target_size:]
    
    print(f"  ì„ íƒëœ ìƒ˜í”Œ: {len(selected_indices)}")
    print(f"  Importance ë¶„í¬: min={importance_scores.min():.6f}, max={importance_scores.max():.6f}")
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[selected_indices])
    print(f"  â†’ FID: {fid:.4f} (í‰ê· : {mean_t:.4f}, ê³µë¶„ì‚°: {cov_t:.4f})")
    
    return fid, mean_t, cov_t, selected_indices


def strategy_combined_v2(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    target_cov_term: float = 17.0,
    device: str = 'cuda'
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ v4-5: 3ë‹¨ê³„ ë³µí•© ì „ëµ
    Stage 1: dimension_targeted (ìƒìœ„ 10% ì œê±°)
    Stage 2: fine_iterative (0.5%ì”© ì •ë°€ ì œê±°)
    Stage 3: eigenspace ê¸°ë°˜ ìµœì¢… ì¡°ì •
    """
    print(f"\n[v4-5] Combined v2 (ëª©í‘œ ê³µë¶„ì‚°: {target_cov_term})")
    print("=" * 60)
    
    # Stage 1: Dimension Targeted Outlier Removal
    print("\n  === Stage 1: Dimension Targeted ===")
    _, _, _, stage1_indices = strategy_dimension_targeted(
        features_A, features_B,
        top_k_dims=200,
        outlier_percentile=90
    )
    
    stage1_features = features_B[stage1_indices]
    fid1, mean1, cov1 = compute_fid(features_A, stage1_features)
    print(f"  Stage 1 ê²°ê³¼: FID={fid1:.4f}, ê³µë¶„ì‚°={cov1:.4f}")
    
    # Stage 2: Fine Iterative on Stage 1 result
    print("\n  === Stage 2: Fine Iterative ===")
    
    # Stage 1ì˜ ì¸ë±ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒˆë¡œìš´ features ìƒì„±
    current_indices = stage1_indices.copy()
    min_samples = int(len(stage1_indices) * 0.5)
    
    best_fid = fid1
    best_cov = cov1
    best_indices = stage1_indices.copy()
    
    no_improve_count = 0
    patience = 15
    iteration = 0
    
    while len(current_indices) > min_samples:
        iteration += 1
        current_features = features_B[current_indices]
        
        mu_A = np.mean(features_A, axis=0)
        mahal_dist = compute_mahalanobis_distances(features_A, current_features)
        mean_contribution = np.sum((current_features - mu_A) ** 2, axis=1)
        
        # ê³µë¶„ì‚°ì— ë” ì§‘ì¤‘
        combined_score = 0.2 * mean_contribution + 0.8 * (mahal_dist ** 2)
        
        n_remove = max(1, int(len(current_indices) * 0.005))
        remove_local_indices = np.argsort(combined_score)[-n_remove:]
        
        keep_mask = np.ones(len(current_indices), dtype=bool)
        keep_mask[remove_local_indices] = False
        current_indices = current_indices[keep_mask]
        
        new_fid, new_mean, new_cov = compute_fid(features_A, features_B[current_indices])
        
        if new_fid < best_fid:
            best_fid = new_fid
            best_cov = new_cov
            best_indices = current_indices.copy()
            no_improve_count = 0
            
            if iteration % 30 == 0:
                print(f"    Iter {iteration}: FID={new_fid:.4f}, cov={new_cov:.4f} â¬‡ï¸")
        else:
            no_improve_count += 1
        
        if new_cov <= target_cov_term:
            print(f"    ğŸ¯ ëª©í‘œ ê³µë¶„ì‚° ë‹¬ì„±! cov={new_cov:.4f}")
            break
        
        if no_improve_count >= patience:
            print(f"    [Early Stop] {patience}íšŒ ë¯¸ê°œì„ ")
            break
    
    fid2, mean2, cov2 = compute_fid(features_A, features_B[best_indices])
    print(f"  Stage 2 ê²°ê³¼: FID={fid2:.4f}, ê³µë¶„ì‚°={cov2:.4f}")
    
    # Stage 3: Eigenspace ê¸°ë°˜ ì¶”ê°€ í•„í„°ë§ (ê³µë¶„ì‚°ì´ ì•„ì§ ëª©í‘œ ë¯¸ë‹¬ì´ë©´)
    if cov2 > target_cov_term:
        print("\n  === Stage 3: Eigenspace Refinement ===")
        
        from sklearn.decomposition import PCA
        pca = PCA(n_components=30)
        pca.fit(features_A)
        
        A_pca = pca.transform(features_A)
        B_pca = pca.transform(features_B[best_indices])
        
        mean_A_pca = np.mean(A_pca, axis=0)
        var_A_pca = np.var(A_pca, axis=0)
        
        B_centered = B_pca - mean_A_pca
        sample_sq_dev = B_centered ** 2
        var_contribution = sample_sq_dev / (var_A_pca + 1e-10)
        
        # ë¶„ì‚° ë¹„ìœ¨ì´ 1ì— ê°€ê¹Œìš´ ìƒ˜í”Œ ì„ íƒ
        deviation = np.abs(var_contribution - 1.0)
        weights = var_A_pca / var_A_pca.sum()
        weighted_dev = np.sum(deviation * weights, axis=1)
        
        # í•˜ìœ„ 80% ì„ íƒ
        threshold = np.percentile(weighted_dev, 80)
        stage3_mask = weighted_dev <= threshold
        stage3_local_indices = np.where(stage3_mask)[0]
        
        final_indices = best_indices[stage3_local_indices]
        fid3, mean3, cov3 = compute_fid(features_A, features_B[final_indices])
        
        print(f"  Stage 3 ê²°ê³¼: FID={fid3:.4f}, ê³µë¶„ì‚°={cov3:.4f}")
        
        if fid3 < fid2:
            best_indices = final_indices
            best_fid = fid3
            best_cov = cov3
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[best_indices])
    print("\n" + "=" * 60)
    print(f"  ğŸ† ìµœì¢…: FID={fid:.4f} (í‰ê· ={mean_t:.4f}, ê³µë¶„ì‚°={cov_t:.4f})")
    print(f"  â†’ ìƒ˜í”Œ ìˆ˜: {len(best_indices)} ({len(best_indices)/len(features_B)*100:.1f}%)")
    
    return fid, mean_t, cov_t, best_indices


def strategy_variance_ratio_filter(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    target_ratio_range: Tuple[float, float] = (0.8, 1.2)
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ v4-6: Variance Ratio Filtering
    ê° ì°¨ì›ì—ì„œ Bê°€ Aì™€ ë¹„ìŠ·í•œ ë¶„ì‚° ê¸°ì—¬ë¥¼ í•˜ëŠ” ìƒ˜í”Œë§Œ ì„ íƒ
    """
    print(f"\n[v4-6] Variance Ratio Filter (range={target_ratio_range})")
    
    mu_A = np.mean(features_A, axis=0)
    var_A = np.var(features_A, axis=0)
    
    # ê° B ìƒ˜í”Œì´ ê° ì°¨ì›ì—ì„œ ë¶„ì‚°ì— ê¸°ì—¬í•˜ëŠ” ì •ë„
    B_centered = features_B - mu_A
    B_sq_dev = B_centered ** 2
    
    # ì´ìƒì  ê¸°ì—¬ë„ (Aì˜ ë¶„ì‚°)
    # B ìƒ˜í”Œì´ Aì˜ ë¶„ì‚°ì— ë§ëŠ”ì§€ í™•ì¸
    # ê° ì°¨ì›ë³„ë¡œ (B_sq_dev / var_A)ê°€ 1ì— ê°€ê¹Œìš°ë©´ ì¢‹ìŒ
    contribution_ratio = B_sq_dev / (var_A + 1e-10)
    
    # ë¶„ì‚°ì´ í° ì°¨ì›ë“¤ (ìƒìœ„ 200ê°œ)ì— ì§‘ì¤‘
    top_dims = np.argsort(var_A)[-200:]
    
    # í•´ë‹¹ ì°¨ì›ë“¤ì—ì„œì˜ ratio
    top_ratio = contribution_ratio[:, top_dims]
    
    # target_ratio_range ë‚´ì— ìˆëŠ” ì°¨ì›ì˜ ë¹„ìœ¨
    in_range = (top_ratio >= target_ratio_range[0]) & (top_ratio <= target_ratio_range[1])
    in_range_ratio = np.mean(in_range, axis=1)
    
    # 80% ì´ìƒì˜ ì°¨ì›ì´ ë²”ìœ„ ë‚´ì¸ ìƒ˜í”Œ ì„ íƒ
    threshold = 0.6
    selected_mask = in_range_ratio >= threshold
    
    if selected_mask.sum() < 1000:
        # ë„ˆë¬´ ì ìœ¼ë©´ threshold ë‚®ì¶¤
        threshold = np.percentile(in_range_ratio, 50)
        selected_mask = in_range_ratio >= threshold
    
    selected_indices = np.where(selected_mask)[0]
    
    print(f"  ì„ íƒëœ ìƒ˜í”Œ: {len(selected_indices)} / {len(features_B)}")
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[selected_indices])
    print(f"  â†’ FID: {fid:.4f} (í‰ê· : {mean_t:.4f}, ê³µë¶„ì‚°: {cov_t:.4f})")
    
    return fid, mean_t, cov_t, selected_indices


# ============================================================
# ğŸ¯ FID ìµœì í™” v5 - ëª©í‘œ: FID < 20
# ============================================================

def strategy_minibatch_sinkhorn_cpu(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    target_size: int = 20000,
    batch_a: int = 500,
    batch_b: int = 2000,
    n_iter: int = 30,
    reg: float = 0.1
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ v5-1: CPU Mini-batch Sinkhorn (OOM ìˆ˜ì • ë²„ì „)
    - Aì—ì„œ batch_aê°œ, Bì—ì„œ batch_bê°œì”© ìƒ˜í”Œë§
    - Cost matrix: batch_a x batch_b (ì‘ì€ í¬ê¸°)
    """
    print(f"\n[v5-1] Minibatch Sinkhorn CPU (a={batch_a}, b={batch_b}, iter={n_iter})")
    
    try:
        import ot
    except ImportError:
        print("  âš ï¸ POT ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”: pip install POT")
        return float('inf'), 0, 0, np.array([])
    
    importance = np.zeros(len(features_B))
    
    for i in tqdm(range(n_iter), desc="  Minibatch OT"):
        np.random.seed(i)
        idx_A = np.random.choice(len(features_A), batch_a, replace=False)
        idx_B = np.random.choice(len(features_B), batch_b, replace=False)
        
        A_batch = features_A[idx_A]
        B_batch = features_B[idx_B]
        
        # ì‘ì€ cost matrix (batch_a x batch_b)
        M = np.linalg.norm(A_batch[:, None] - B_batch[None, :], axis=2)
        M = M / (M.max() + 1e-8)  # ì •ê·œí™”
        
        a = np.ones(batch_a) / batch_a
        b = np.ones(batch_b) / batch_b
        
        try:
            T = ot.sinkhorn(a, b, M, reg=reg, numItermax=50, stopThr=1e-6)
            importance[idx_B] += T.sum(axis=0)
        except Exception as e:
            continue
    
    selected = np.argsort(importance)[-target_size:]
    
    print(f"  ì„ íƒëœ ìƒ˜í”Œ: {len(selected)}")
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[selected])
    print(f"  â†’ FID: {fid:.4f} (í‰ê· : {mean_t:.4f}, ê³µë¶„ì‚°: {cov_t:.4f})")
    
    return fid, mean_t, cov_t, selected


def strategy_dimtarget_grid_search(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    top_k_range: List[int] = None,
    percentile_range: List[int] = None
) -> Tuple[float, float, float, np.ndarray, dict]:
    """
    ì „ëµ v5-2: DimTarget íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„œì¹˜
    - top_k: 50~150 ë²”ìœ„ì—ì„œ 25 ë‹¨ìœ„ë¡œ
    - percentile: 80~90 ë²”ìœ„ì—ì„œ 2 ë‹¨ìœ„ë¡œ
    """
    if top_k_range is None:
        top_k_range = [50, 75, 100, 125, 150]
    if percentile_range is None:
        percentile_range = list(range(80, 91, 2))
    
    print(f"\n[v5-2] DimTarget Grid Search")
    print(f"  top_k ë²”ìœ„: {top_k_range}")
    print(f"  percentile ë²”ìœ„: {percentile_range}")
    print("-" * 70)
    
    # ì°¨ì›ë³„ ë¶„ì‚° ê³„ì‚° (í•œ ë²ˆë§Œ)
    var_A = np.var(features_A, axis=0)
    var_B = np.var(features_B, axis=0)
    var_ratio = var_B / (var_A + 1e-10)
    var_diff = np.abs(var_ratio - 1.0)
    
    results = {}
    best_fid = float('inf')
    best_params = None
    best_indices = None
    
    for top_k in top_k_range:
        target_dims = np.argsort(var_diff)[-top_k:]
        
        mu_A_target = np.mean(features_A[:, target_dims], axis=0)
        std_A_target = np.std(features_A[:, target_dims], axis=0) + 1e-10
        
        B_target = features_B[:, target_dims]
        z_scores = np.abs((B_target - mu_A_target) / std_A_target)
        outlier_scores = np.mean(z_scores, axis=1)
        
        for percentile in percentile_range:
            threshold = np.percentile(outlier_scores, percentile)
            selected_indices = np.where(outlier_scores <= threshold)[0]
            
            if len(selected_indices) < 1000:
                continue
            
            fid, mean_t, cov_t = compute_fid(features_A, features_B[selected_indices])
            results[(top_k, percentile)] = {'fid': fid, 'mean': mean_t, 'cov': cov_t, 'n': len(selected_indices)}
            
            if fid < best_fid:
                best_fid = fid
                best_params = (top_k, percentile)
                best_indices = selected_indices.copy()
                print(f"  âœ¨ k={top_k:3d}, p={percentile:2d}: FID={fid:.4f} (c={cov_t:.4f}) n={len(selected_indices)}")
    
    print("-" * 70)
    print(f"  ğŸ† ìµœì : k={best_params[0]}, p={best_params[1]}, FID={best_fid:.4f}")
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[best_indices])
    return fid, mean_t, cov_t, best_indices, results


def strategy_dimtarget_then_iterative(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    top_k: int = 100,
    dim_percentile: int = 85,
    iter_rate: float = 0.002,
    iter_patience: int = 20
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ v5-3: DimTarget + Fine Iterative ì¡°í•©
    Stage 1: Dimension Targetedë¡œ 1ì°¨ í•„í„°ë§
    Stage 2: ë” ì„¸ë°€í•œ Iterative (0.2%ì”©)
    """
    print(f"\n[v5-3] DimTarget + Iterative (k={top_k}, p={dim_percentile}, rate={iter_rate})")
    
    # Stage 1: Dimension Targeted
    var_A = np.var(features_A, axis=0)
    var_B = np.var(features_B, axis=0)
    var_ratio = var_B / (var_A + 1e-10)
    var_diff = np.abs(var_ratio - 1.0)
    target_dims = np.argsort(var_diff)[-top_k:]
    
    mu_A_target = np.mean(features_A[:, target_dims], axis=0)
    std_A_target = np.std(features_A[:, target_dims], axis=0) + 1e-10
    
    B_target = features_B[:, target_dims]
    z_scores = np.abs((B_target - mu_A_target) / std_A_target)
    outlier_scores = np.mean(z_scores, axis=1)
    
    threshold = np.percentile(outlier_scores, dim_percentile)
    stage1_indices = np.where(outlier_scores <= threshold)[0]
    
    fid1, mean1, cov1 = compute_fid(features_A, features_B[stage1_indices])
    print(f"  Stage 1: n={len(stage1_indices)}, FID={fid1:.4f}, cov={cov1:.4f}")
    
    # Stage 2: Fine Iterative
    current_indices = stage1_indices.copy()
    min_samples = int(len(stage1_indices) * 0.5)
    
    best_fid = fid1
    best_indices = stage1_indices.copy()
    no_improve_count = 0
    iteration = 0
    
    while len(current_indices) > min_samples:
        iteration += 1
        current_features = features_B[current_indices]
        
        mu_A = np.mean(features_A, axis=0)
        mahal_dist = compute_mahalanobis_distances(features_A, current_features)
        mean_contribution = np.sum((current_features - mu_A) ** 2, axis=1)
        
        combined_score = 0.3 * mean_contribution + 0.7 * (mahal_dist ** 2)
        
        n_remove = max(1, int(len(current_indices) * iter_rate))
        remove_local_indices = np.argsort(combined_score)[-n_remove:]
        
        keep_mask = np.ones(len(current_indices), dtype=bool)
        keep_mask[remove_local_indices] = False
        current_indices = current_indices[keep_mask]
        
        new_fid, new_mean, new_cov = compute_fid(features_A, features_B[current_indices])
        
        if new_fid < best_fid:
            best_fid = new_fid
            best_indices = current_indices.copy()
            no_improve_count = 0
            
            if iteration % 50 == 0:
                print(f"    Iter {iteration}: FID={new_fid:.4f}, cov={new_cov:.4f} â¬‡ï¸")
        else:
            no_improve_count += 1
        
        if no_improve_count >= iter_patience:
            break
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[best_indices])
    print(f"  Stage 2: n={len(best_indices)}, FID={fid:.4f}, cov={cov_t:.4f}")
    
    return fid, mean_t, cov_t, best_indices


def strategy_asymmetric_dim(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    under_threshold: float = 0.9,
    over_threshold: float = 1.1,
    percentile: int = 85
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ v5-4: Asymmetric Dimension Targeting
    ë¶„ì‚°ì´ 'ë¶€ì¡±í•œ' ì°¨ì›ê³¼ 'ê³¼ì‰ì¸' ì°¨ì›ì„ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
    - ë¶€ì¡± ì°¨ì›: ì¤‘ì‹¬ì—ì„œ ë¨¼ ìƒ˜í”Œ ìœ ì§€ (ë¶„ì‚° ì¦ê°€)
    - ê³¼ì‰ ì°¨ì›: ê·¹ë‹¨ê°’ ì œê±° (ë¶„ì‚° ê°ì†Œ)
    """
    print(f"\n[v5-4] Asymmetric Dim (under<{under_threshold}, over>{over_threshold})")
    
    var_A = np.var(features_A, axis=0)
    var_B = np.var(features_B, axis=0)
    ratio = var_B / (var_A + 1e-8)
    
    under_dims = np.where(ratio < under_threshold)[0]
    over_dims = np.where(ratio > over_threshold)[0]
    
    print(f"  ë¶„ì‚° ë¶€ì¡± ì°¨ì›: {len(under_dims)}ê°œ")
    print(f"  ë¶„ì‚° ê³¼ì‰ ì°¨ì›: {len(over_dims)}ê°œ")
    
    scores = np.zeros(len(features_B))
    
    # ê³¼ì‰ ì°¨ì›: ê·¹ë‹¨ê°’ì— í˜ë„í‹° (ì œê±° ëŒ€ìƒ)
    for dim in over_dims:
        mu = np.mean(features_A[:, dim])
        sigma = np.std(features_A[:, dim]) + 1e-6
        deviation = np.abs(features_B[:, dim] - mu) / sigma
        scores += deviation
    
    # ë¶€ì¡± ì°¨ì›: ì¤‘ì‹¬ì—ì„œ ë¨¼ ìƒ˜í”Œì— ë³´ë„ˆìŠ¤ (ìœ ì§€ ëŒ€ìƒ) - ìŒìˆ˜ ì ìˆ˜
    for dim in under_dims:
        mu = np.mean(features_A[:, dim])
        sigma = np.std(features_A[:, dim]) + 1e-6
        deviation = np.abs(features_B[:, dim] - mu) / sigma
        # ì ë‹¹íˆ ë¨¼ ìƒ˜í”Œì€ ìœ ì§€ (ê·¹ë‹¨ì€ ì œì™¸)
        bonus = np.clip(deviation, 0, 2)  # 2 sigmaê¹Œì§€ë§Œ ë³´ë„ˆìŠ¤
        scores -= bonus * 0.3  # ì•½í•œ ë³´ë„ˆìŠ¤
    
    # ìƒìœ„ N% ì œê±° (ì ìˆ˜ê°€ ë†’ì€ ê²ƒ)
    threshold = np.percentile(scores, percentile)
    selected = np.where(scores <= threshold)[0]
    
    print(f"  ì„ íƒëœ ìƒ˜í”Œ: {len(selected)} / {len(features_B)}")
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[selected])
    print(f"  â†’ FID: {fid:.4f} (í‰ê· : {mean_t:.4f}, ê³µë¶„ì‚°: {cov_t:.4f})")
    
    return fid, mean_t, cov_t, selected


def strategy_cluster_adaptive(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    n_clusters: int = 30
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ v5-5: Per-Cluster Adaptive Filtering
    í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë‹¤ë¥¸ í•„í„°ë§ ê°•ë„ ì ìš©
    - Aì— ë¹„í•´ Bê°€ ë¶€ì¡±í•œ í´ëŸ¬ìŠ¤í„°: ëœ í•„í„°ë§
    - Aì— ë¹„í•´ Bê°€ ê³¼ì‰ì¸ í´ëŸ¬ìŠ¤í„°: ë” ê°•í•˜ê²Œ í•„í„°ë§
    """
    print(f"\n[v5-5] Cluster Adaptive (n_clusters={n_clusters})")
    
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    kmeans.fit(features_A)
    
    labels_A = kmeans.labels_
    labels_B = kmeans.predict(features_B)
    
    ratio_A = np.bincount(labels_A, minlength=n_clusters) / len(features_A)
    ratio_B = np.bincount(labels_B, minlength=n_clusters) / len(features_B)
    
    selected = []
    
    for c in range(n_clusters):
        cluster_indices = np.where(labels_B == c)[0]
        
        if len(cluster_indices) == 0:
            continue
        
        excess_ratio = ratio_B[c] / (ratio_A[c] + 1e-8)
        
        if excess_ratio > 1.5:
            keep_ratio = 0.6
        elif excess_ratio > 1.2:
            keep_ratio = 0.75
        elif excess_ratio < 0.7:
            keep_ratio = 0.95
        elif excess_ratio < 0.9:
            keep_ratio = 0.9
        else:
            keep_ratio = 0.85
        
        # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì— ê°€ê¹Œìš´ ìˆœìœ¼ë¡œ ì„ íƒ
        center = kmeans.cluster_centers_[c]
        cluster_features = features_B[cluster_indices]
        dists = np.linalg.norm(cluster_features - center, axis=1)
        n_keep = max(1, int(len(cluster_indices) * keep_ratio))
        selected.extend(cluster_indices[np.argsort(dists)[:n_keep]].tolist())
    
    selected = np.array(selected)
    print(f"  ì„ íƒëœ ìƒ˜í”Œ: {len(selected)} / {len(features_B)}")
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[selected])
    print(f"  â†’ FID: {fid:.4f} (í‰ê· : {mean_t:.4f}, ê³µë¶„ì‚°: {cov_t:.4f})")
    
    return fid, mean_t, cov_t, selected


def strategy_ensemble_selection(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    min_votes: int = 2
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ v5-6: Ensemble Selection
    ì—¬ëŸ¬ ì „ëµì˜ íˆ¬í‘œë¡œ ì„ íƒ (min_votesê°œ ì´ìƒ ì „ëµì—ì„œ ì„ íƒëœ ìƒ˜í”Œë§Œ)
    """
    print(f"\n[v5-6] Ensemble Selection (min_votes={min_votes})")
    
    votes = np.zeros(len(features_B))
    
    # ì „ëµ 1: DimTarget (k=100, p=85)
    var_A = np.var(features_A, axis=0)
    var_B = np.var(features_B, axis=0)
    var_diff = np.abs(var_B / (var_A + 1e-10) - 1.0)
    target_dims = np.argsort(var_diff)[-100:]
    mu_A_t = np.mean(features_A[:, target_dims], axis=0)
    std_A_t = np.std(features_A[:, target_dims], axis=0) + 1e-10
    z1 = np.mean(np.abs((features_B[:, target_dims] - mu_A_t) / std_A_t), axis=1)
    idx1 = np.where(z1 <= np.percentile(z1, 85))[0]
    votes[idx1] += 1
    print(f"  ì „ëµ1 (DimTarget k=100,p=85): {len(idx1)}ê°œ")
    
    # ì „ëµ 2: DimTarget (k=75, p=87)
    target_dims2 = np.argsort(var_diff)[-75:]
    mu_A_t2 = np.mean(features_A[:, target_dims2], axis=0)
    std_A_t2 = np.std(features_A[:, target_dims2], axis=0) + 1e-10
    z2 = np.mean(np.abs((features_B[:, target_dims2] - mu_A_t2) / std_A_t2), axis=1)
    idx2 = np.where(z2 <= np.percentile(z2, 87))[0]
    votes[idx2] += 1
    print(f"  ì „ëµ2 (DimTarget k=75,p=87): {len(idx2)}ê°œ")
    
    # ì „ëµ 3: Mahalanobis (p=90)
    mahal_dist = compute_mahalanobis_distances(features_A, features_B)
    idx3 = np.where(mahal_dist <= np.percentile(mahal_dist, 90))[0]
    votes[idx3] += 1
    print(f"  ì „ëµ3 (Mahalanobis p=90): {len(idx3)}ê°œ")
    
    # ì „ëµ 4: Asymmetric
    ratio = var_B / (var_A + 1e-8)
    over_dims = np.where(ratio > 1.1)[0]
    scores = np.zeros(len(features_B))
    for dim in over_dims:
        mu = np.mean(features_A[:, dim])
        sigma = np.std(features_A[:, dim]) + 1e-6
        scores += np.abs(features_B[:, dim] - mu) / sigma
    idx4 = np.where(scores <= np.percentile(scores, 85))[0]
    votes[idx4] += 1
    print(f"  ì „ëµ4 (Asymmetric p=85): {len(idx4)}ê°œ")
    
    # min_votes ì´ìƒ ì„ íƒëœ ìƒ˜í”Œ
    selected = np.where(votes >= min_votes)[0]
    
    print(f"  {min_votes}ê°œ ì´ìƒ íˆ¬í‘œ: {len(selected)} / {len(features_B)}")
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[selected])
    print(f"  â†’ FID: {fid:.4f} (í‰ê· : {mean_t:.4f}, ê³µë¶„ì‚°: {cov_t:.4f})")
    
    return fid, mean_t, cov_t, selected


def strategy_combined_v3(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    target_fid: float = 20.0
) -> Tuple[float, float, float, np.ndarray]:
    """
    ì „ëµ v5-7: ìµœì  ì¡°í•© v3
    DimTarget Grid Search ìµœì  â†’ Fine Iterative
    """
    print(f"\n[v5-7] Combined v3 (ëª©í‘œ FID: {target_fid})")
    print("=" * 60)
    
    # Stage 1: DimTarget ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°
    print("\n  === Stage 1: DimTarget Grid Search ===")
    _, _, _, stage1_indices, grid_results = strategy_dimtarget_grid_search(
        features_A, features_B,
        top_k_range=[50, 75, 100, 125],
        percentile_range=[83, 85, 87, 89]
    )
    
    fid1, mean1, cov1 = compute_fid(features_A, features_B[stage1_indices])
    print(f"  Stage 1 ê²°ê³¼: FID={fid1:.4f}, ê³µë¶„ì‚°={cov1:.4f}")
    
    if fid1 <= target_fid:
        print(f"  ğŸ‰ Stage 1ì—ì„œ ëª©í‘œ ë‹¬ì„±!")
        return fid1, mean1, cov1, stage1_indices
    
    # Stage 2: Fine Iterative
    print("\n  === Stage 2: Ultra-Fine Iterative ===")
    current_indices = stage1_indices.copy()
    min_samples = int(len(stage1_indices) * 0.4)
    
    best_fid = fid1
    best_indices = stage1_indices.copy()
    no_improve_count = 0
    patience = 25
    iteration = 0
    
    while len(current_indices) > min_samples:
        iteration += 1
        current_features = features_B[current_indices]
        
        mu_A = np.mean(features_A, axis=0)
        mahal_dist = compute_mahalanobis_distances(features_A, current_features)
        mean_contribution = np.sum((current_features - mu_A) ** 2, axis=1)
        
        combined_score = 0.25 * mean_contribution + 0.75 * (mahal_dist ** 2)
        
        n_remove = max(1, int(len(current_indices) * 0.001))  # 0.1%ì”©
        remove_local_indices = np.argsort(combined_score)[-n_remove:]
        
        keep_mask = np.ones(len(current_indices), dtype=bool)
        keep_mask[remove_local_indices] = False
        current_indices = current_indices[keep_mask]
        
        new_fid, new_mean, new_cov = compute_fid(features_A, features_B[current_indices])
        
        if new_fid < best_fid:
            best_fid = new_fid
            best_indices = current_indices.copy()
            no_improve_count = 0
            
            if iteration % 100 == 0 or new_fid < target_fid:
                print(f"    Iter {iteration}: FID={new_fid:.4f} â¬‡ï¸")
        else:
            no_improve_count += 1
        
        if new_fid <= target_fid:
            print(f"    ğŸ‰ ëª©í‘œ FID ë‹¬ì„±!")
            break
        
        if no_improve_count >= patience:
            break
    
    fid, mean_t, cov_t = compute_fid(features_A, features_B[best_indices])
    print("\n" + "=" * 60)
    print(f"  ğŸ† ìµœì¢…: FID={fid:.4f} (í‰ê· ={mean_t:.4f}, ê³µë¶„ì‚°={cov_t:.4f})")
    print(f"  â†’ ìƒ˜í”Œ ìˆ˜: {len(best_indices)} ({len(best_indices)/len(features_B)*100:.1f}%)")
    
    return fid, mean_t, cov_t, best_indices


def evaluate_all_strategies_v4(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    device: str = 'cuda'
) -> List[Tuple]:
    """ëª¨ë“  v5 ì „ëµ í‰ê°€ ë° ë¹„êµ (ëª©í‘œ: FID < 20)"""
    
    print("\n" + "=" * 95)
    print("ğŸ† FID ê°œì„  ì „ëµ ë¹„êµ (v5 - ëª©í‘œ: FID < 20)")
    print("=" * 95)
    
    original_fid, orig_mean, orig_cov = compute_fid(features_A, features_B)
    print(f"\nğŸ“Š ì›ë³¸ FID (B ì „ì²´ {len(features_B)}ê°œ): {original_fid:.4f}")
    print(f"   (í‰ê· : {orig_mean:.4f}, ê³µë¶„ì‚°: {orig_cov:.4f})")
    
    results = [("ì›ë³¸ (B ì „ì²´)", original_fid, orig_mean, orig_cov, len(features_B), None)]
    
    # v5-1: Minibatch Sinkhorn CPU
    print("\n" + "=" * 95)
    try:
        fid1, mean1, cov1, idx1 = strategy_minibatch_sinkhorn_cpu(
            features_A, features_B,
            target_size=len(features_A) * 4,
            batch_a=500, batch_b=2000, n_iter=30
        )
        results.append(("Sinkhorn CPU", fid1, mean1, cov1, len(idx1), idx1))
    except Exception as e:
        print(f"  â†’ ì‹¤íŒ¨: {e}")
    
    # v5-2: DimTarget Grid Search
    print("\n" + "=" * 95)
    try:
        fid2, mean2, cov2, idx2, _ = strategy_dimtarget_grid_search(
            features_A, features_B,
            top_k_range=[50, 75, 100, 125, 150],
            percentile_range=[80, 82, 84, 85, 86, 88, 90]
        )
        results.append(("DimTarget GridSearch", fid2, mean2, cov2, len(idx2), idx2))
    except Exception as e:
        print(f"  â†’ ì‹¤íŒ¨: {e}")
    
    # v5-3: DimTarget + Iterative (ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°)
    print("\n" + "=" * 95)
    for k, p in [(75, 86), (100, 85), (100, 84)]:
        try:
            fid3, mean3, cov3, idx3 = strategy_dimtarget_then_iterative(
                features_A, features_B,
                top_k=k, dim_percentile=p,
                iter_rate=0.002, iter_patience=25
            )
            results.append((f"DimTarget+Iter(k={k},p={p})", fid3, mean3, cov3, len(idx3), idx3))
        except Exception as e:
            pass
    
    # v5-4: Asymmetric Dim
    print("\n" + "=" * 95)
    for percentile in [83, 85, 87]:
        try:
            fid4, mean4, cov4, idx4 = strategy_asymmetric_dim(
                features_A, features_B,
                percentile=percentile
            )
            results.append((f"Asymmetric(p={percentile})", fid4, mean4, cov4, len(idx4), idx4))
        except Exception as e:
            pass
    
    # v5-5: Cluster Adaptive
    print("\n" + "=" * 95)
    for n_c in [20, 30, 50]:
        try:
            fid5, mean5, cov5, idx5 = strategy_cluster_adaptive(
                features_A, features_B,
                n_clusters=n_c
            )
            results.append((f"ClusterAdaptive(c={n_c})", fid5, mean5, cov5, len(idx5), idx5))
        except Exception as e:
            pass
    
    # v5-6: Ensemble
    print("\n" + "=" * 95)
    for min_v in [2, 3]:
        try:
            fid6, mean6, cov6, idx6 = strategy_ensemble_selection(
                features_A, features_B,
                min_votes=min_v
            )
            results.append((f"Ensemble(v>={min_v})", fid6, mean6, cov6, len(idx6), idx6))
        except Exception as e:
            pass
    
    # v5-7: Combined v3
    print("\n" + "=" * 95)
    try:
        fid7, mean7, cov7, idx7 = strategy_combined_v3(
            features_A, features_B,
            target_fid=20.0
        )
        results.append(("Combined v3", fid7, mean7, cov7, len(idx7), idx7))
    except Exception as e:
        print(f"  â†’ ì‹¤íŒ¨: {e}")
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 95)
    print("ğŸ“‹ ì „ëµë³„ ê²°ê³¼ ìš”ì•½ (FID ìˆœ ì •ë ¬)")
    print("=" * 95)
    print(f"\n{'ì „ëµ':<35} {'FID':>8} {'í‰ê· ':>8} {'ê³µë¶„ì‚°':>10} {'ìƒ˜í”Œìˆ˜':>10} {'vsì›ë³¸':>10}")
    print("-" * 95)
    
    results.sort(key=lambda x: x[1])
    
    for i, (name, fid, mean_t, cov_t, n_samples, _) in enumerate(results[:25]):
        diff = fid - original_fid
        fid_marker = "ğŸ†" if i == 0 and name != "ì›ë³¸ (B ì „ì²´)" else "  "
        goal_marker = "âœ…" if fid < 20 else "  "
        print(f"{fid_marker}{goal_marker}{name:<32} {fid:>8.4f} {mean_t:>8.4f} {cov_t:>10.4f} {n_samples:>10} {diff:>+10.4f}")
    
    # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
    under_20 = [(n, f, m, c, s, idx) for n, f, m, c, s, idx in results if f < 20 and n != "ì›ë³¸ (B ì „ì²´)"]
    
    print("\n" + "=" * 95)
    if under_20:
        print(f"ğŸ‰ FID < 20 ë‹¬ì„±: {len(under_20)}ê°œ ì „ëµ!")
        best = min(under_20, key=lambda x: x[1])
        print(f"   ìµœì € FID: {best[0]} (FID={best[1]:.4f})")
    else:
        best = min(results, key=lambda x: x[1] if x[0] != "ì›ë³¸ (B ì „ì²´)" else float('inf'))
        print(f"âš ï¸ ëª©í‘œ ë¯¸ë‹¬. ìµœê³ : {best[0]} (FID={best[1]:.4f})")
        print(f"   ëª©í‘œ(20)ê¹Œì§€: {best[1] - 20:.4f} ë‚¨ìŒ")
    
    return results


def evaluate_all_strategies_v3(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    device: str = 'cuda'
) -> List[Tuple]:
    """ëª¨ë“  v4 ì „ëµ í‰ê°€ ë° ë¹„êµ (ê³µë¶„ì‚° term 17 ì´í•˜ ëª©í‘œ)"""
    
    print("\n" + "=" * 90)
    print("ğŸ† FID ê°œì„  ì „ëµ ë¹„êµ (v4 - ëª©í‘œ: FID < 20, ê³µë¶„ì‚° term < 17)")
    print("=" * 90)
    
    # ì›ë³¸ FID
    original_fid, orig_mean, orig_cov = compute_fid(features_A, features_B)
    print(f"\nğŸ“Š ì›ë³¸ FID (B ì „ì²´ {len(features_B)}ê°œ): {original_fid:.4f}")
    print(f"   (í‰ê· : {orig_mean:.4f}, ê³µë¶„ì‚°: {orig_cov:.4f})")
    print(f"   ê³µë¶„ì‚° ê¸°ì—¬ë„: {orig_cov/original_fid*100:.1f}%")
    
    results = [("ì›ë³¸ (B ì „ì²´)", original_fid, orig_mean, orig_cov, len(features_B), None)]
    
    # v4-1: Fine Iterative
    print("\n" + "=" * 90)
    try:
        fid1, mean1, cov1, indices1 = strategy_fine_iterative(
            features_A, features_B,
            removal_rate=0.005,
            patience=15,
            target_cov_term=17.0
        )
        results.append(("Fine Iterative (0.5%)", fid1, mean1, cov1, len(indices1), indices1))
    except Exception as e:
        print(f"  â†’ ì‹¤íŒ¨: {e}")
    
    # v4-2: Dimension Targeted (ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°)
    print("\n" + "=" * 90)
    for top_k in [100, 200, 300]:
        for percentile in [90, 85, 80]:
            try:
                fid2, mean2, cov2, indices2 = strategy_dimension_targeted(
                    features_A, features_B,
                    top_k_dims=top_k,
                    outlier_percentile=percentile
                )
                results.append((f"DimTarget(k={top_k},p={percentile})", fid2, mean2, cov2, len(indices2), indices2))
            except Exception as e:
                pass
    
    # v4-3: Eigenspace Variance Match
    print("\n" + "=" * 90)
    for n_comp in [30, 50, 100]:
        for tol in [0.2, 0.3, 0.5]:
            try:
                fid3, mean3, cov3, indices3 = strategy_eigenspace_variance_match(
                    features_A, features_B,
                    n_components=n_comp,
                    tolerance=tol
                )
                results.append((f"EigenVar(c={n_comp},t={tol})", fid3, mean3, cov3, len(indices3), indices3))
            except Exception as e:
                pass
    
    # v4-4: Minibatch Sinkhorn
    print("\n" + "=" * 90)
    try:
        fid4, mean4, cov4, indices4 = strategy_minibatch_sinkhorn(
            features_A, features_B,
            target_size=len(features_A) * 4,
            batch_size=3000,
            n_iter=20,
            reg=0.1,
            device=device
        )
        if len(indices4) > 0:
            results.append(("Minibatch Sinkhorn", fid4, mean4, cov4, len(indices4), indices4))
    except Exception as e:
        print(f"  â†’ ì‹¤íŒ¨: {e}")
    
    # v4-5: Combined v2
    print("\n" + "=" * 90)
    try:
        fid5, mean5, cov5, indices5 = strategy_combined_v2(
            features_A, features_B,
            target_cov_term=17.0,
            device=device
        )
        results.append(("Combined v2 (3-stage)", fid5, mean5, cov5, len(indices5), indices5))
    except Exception as e:
        print(f"  â†’ ì‹¤íŒ¨: {e}")
    
    # v4-6: Variance Ratio Filter
    print("\n" + "=" * 90)
    for low, high in [(0.7, 1.3), (0.8, 1.2), (0.9, 1.1)]:
        try:
            fid6, mean6, cov6, indices6 = strategy_variance_ratio_filter(
                features_A, features_B,
                target_ratio_range=(low, high)
            )
            results.append((f"VarRatio({low}-{high})", fid6, mean6, cov6, len(indices6), indices6))
        except Exception as e:
            pass
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 90)
    print("ğŸ“‹ ì „ëµë³„ ê²°ê³¼ ìš”ì•½ (FID ìˆœ ì •ë ¬)")
    print("=" * 90)
    print(f"\n{'ì „ëµ':<35} {'FID':>8} {'í‰ê· ':>8} {'ê³µë¶„ì‚°':>10} {'ìƒ˜í”Œìˆ˜':>10} {'vsì›ë³¸':>10}")
    print("-" * 95)
    
    # FID ìˆœìœ¼ë¡œ ì •ë ¬
    results.sort(key=lambda x: x[1])
    
    for i, (name, fid, mean_t, cov_t, n_samples, _) in enumerate(results[:20]):
        diff = fid - original_fid
        cov_marker = "âœ…" if cov_t < 17 else "  "
        fid_marker = "ğŸ†" if i == 0 and name != "ì›ë³¸ (B ì „ì²´)" else "  "
        print(f"{fid_marker}{cov_marker}{name:<32} {fid:>8.4f} {mean_t:>8.4f} {cov_t:>10.4f} {n_samples:>10} {diff:>+10.4f}")
    
    # ê³µë¶„ì‚° 17 ì´í•˜ì¸ ê²°ê³¼ë“¤
    cov_under_17 = [(n, f, m, c, s, idx) for n, f, m, c, s, idx in results if c < 17 and n != "ì›ë³¸ (B ì „ì²´)"]
    
    print("\n" + "=" * 90)
    if cov_under_17:
        print(f"âœ… ê³µë¶„ì‚° < 17 ë‹¬ì„±: {len(cov_under_17)}ê°œ ì „ëµ")
        best_cov = min(cov_under_17, key=lambda x: x[3])
        print(f"   ìµœì € ê³µë¶„ì‚°: {best_cov[0]} (cov={best_cov[3]:.4f}, FID={best_cov[1]:.4f})")
    else:
        print("âš ï¸ ê³µë¶„ì‚° < 17 ë‹¬ì„±í•œ ì „ëµ ì—†ìŒ")
        # ê°€ì¥ ë‚®ì€ ê³µë¶„ì‚° ì°¾ê¸°
        best_cov_result = min(results, key=lambda x: x[3] if x[0] != "ì›ë³¸ (B ì „ì²´)" else float('inf'))
        print(f"   í˜„ì¬ ìµœì € ê³µë¶„ì‚°: {best_cov_result[0]} (cov={best_cov_result[3]:.4f})")
    
    # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
    best_result = None
    for r in results:
        if r[0] != "ì›ë³¸ (B ì „ì²´)":
            if best_result is None or r[1] < best_result[1]:
                best_result = r
    
    if best_result:
        print(f"\nğŸ† ìµœê³  FID: {best_result[0]}")
        print(f"   FID: {best_result[1]:.4f} (í‰ê· : {best_result[2]:.4f}, ê³µë¶„ì‚°: {best_result[3]:.4f})")
        if best_result[1] < 20:
            print("   ğŸ‰ ëª©í‘œ ë‹¬ì„±!")
        else:
            print(f"   ëª©í‘œ(20)ê¹Œì§€: {best_result[1] - 20:.4f} ë‚¨ìŒ")
    
    return results


def evaluate_all_strategies_v2(
    features_A: np.ndarray, 
    features_B: np.ndarray,
    device: str = 'cuda'
) -> List[Tuple]:
    """ëª¨ë“  v3 ì „ëµ í‰ê°€ ë° ë¹„êµ"""
    
    print("\n" + "=" * 80)
    print("ğŸ† FID ê°œì„  ì „ëµ ë¹„êµ (v3 - ëª©í‘œ: FID < 20)")
    print("=" * 80)
    
    # ì›ë³¸ FID
    original_fid, orig_mean, orig_cov = compute_fid(features_A, features_B)
    print(f"\nğŸ“Š ì›ë³¸ FID (B ì „ì²´ {len(features_B)}ê°œ): {original_fid:.4f}")
    print(f"   (í‰ê· : {orig_mean:.4f}, ê³µë¶„ì‚°: {orig_cov:.4f})")
    print(f"   ê³µë¶„ì‚° ê¸°ì—¬ë„: {orig_cov/original_fid*100:.1f}%")
    
    results = [("ì›ë³¸ (B ì „ì²´)", original_fid, orig_mean, orig_cov, len(features_B), None)]
    
    # ì „ëµ 1: ê³µê²©ì  Outlier íƒìƒ‰
    print("\n" + "=" * 80)
    try:
        fid1, mean1, cov1, indices1, search_results = strategy_aggressive_outlier_search(
            features_A, features_B
        )
        results.append(("Aggressive Outlier", fid1, mean1, cov1, len(indices1), indices1))
    except Exception as e:
        print(f"  â†’ ì‹¤íŒ¨: {e}")
    
    # ì „ëµ 2: 2ë‹¨ê³„ ë³µí•© (ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°)
    print("\n" + "=" * 80)
    for outlier_p in [90, 85, 80]:
        for target_mult in [2, 4, 8]:
            try:
                target = len(features_A) * target_mult
                fid2, mean2, cov2, indices2 = strategy_two_stage_hybrid(
                    features_A, features_B, 
                    outlier_percentile=outlier_p,
                    target_size=target
                )
                results.append((f"Hybrid(p={outlier_p},Ã—{target_mult})", fid2, mean2, cov2, len(indices2), indices2))
            except Exception as e:
                print(f"  â†’ ì‹¤íŒ¨: {e}")
    
    # ì „ëµ 3: Iterative Removal
    print("\n" + "=" * 80)
    try:
        fid3, mean3, cov3, indices3 = strategy_iterative_removal(
            features_A, features_B,
            target_fid=20.0,
            max_remove_ratio=0.4,
            removal_rate=0.02
        )
        results.append(("Iterative Removal", fid3, mean3, cov3, len(indices3), indices3))
    except Exception as e:
        print(f"  â†’ ì‹¤íŒ¨: {e}")
    
    # ì „ëµ 4: ê³µë¶„ì‚° Greedy
    print("\n" + "=" * 80)
    try:
        fid4, mean4, cov4, indices4 = strategy_covariance_greedy(
            features_A, features_B,
            target_size=len(features_A) * 4,
            n_iter=200
        )
        results.append(("Covariance Greedy", fid4, mean4, cov4, len(indices4), indices4))
    except Exception as e:
        print(f"  â†’ ì‹¤íŒ¨: {e}")
    
    # ì „ëµ 5: Sinkhorn OT
    print("\n" + "=" * 80)
    try:
        for reg in [0.1, 0.05, 0.01]:
            fid5, mean5, cov5, indices5 = strategy_sinkhorn_ot(
                features_A, features_B,
                target_size=len(features_A) * 4,
                reg=reg,
                device=device
            )
            if len(indices5) > 0:
                results.append((f"Sinkhorn OT(reg={reg})", fid5, mean5, cov5, len(indices5), indices5))
    except Exception as e:
        print(f"  â†’ ì‹¤íŒ¨: {e}")
    
    # ì „ëµ 6: ê³ ìœ ê°’ ë§¤ì¹­
    print("\n" + "=" * 80)
    try:
        fid6, mean6, cov6, indices6 = strategy_eigenvalue_matching(
            features_A, features_B,
            target_size=len(features_A) * 4
        )
        results.append(("Eigenvalue Matching", fid6, mean6, cov6, len(indices6), indices6))
    except Exception as e:
        print(f"  â†’ ì‹¤íŒ¨: {e}")
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 80)
    print("ğŸ“‹ ì „ëµë³„ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    print(f"\n{'ì „ëµ':<30} {'FID':>10} {'í‰ê· ':>10} {'ê³µë¶„ì‚°':>12} {'ìƒ˜í”Œìˆ˜':>10} {'vsì›ë³¸':>10}")
    print("-" * 90)
    
    # FID ìˆœìœ¼ë¡œ ì •ë ¬
    results.sort(key=lambda x: x[1])
    
    for name, fid, mean_t, cov_t, n_samples, _ in results:
        diff = fid - original_fid
        marker = "ğŸ†" if fid == results[0][1] and name != "ì›ë³¸ (B ì „ì²´)" else ""
        print(f"{marker}{name:<28} {fid:>10.4f} {mean_t:>10.4f} {cov_t:>12.4f} {n_samples:>10} {diff:>+10.4f}")
    
    # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
    best_result = None
    for r in results:
        if r[0] != "ì›ë³¸ (B ì „ì²´)":
            if best_result is None or r[1] < best_result[1]:
                best_result = r
    
    print("\n" + "=" * 80)
    if best_result:
        if best_result[1] < 20:
            print(f"ğŸ‰ ëª©í‘œ ë‹¬ì„±! ìµœê³  FID: {best_result[1]:.4f} ({best_result[0]})")
        elif best_result[1] < original_fid:
            print(f"ğŸ“ˆ ê°œì„ ë¨! ìµœê³  FID: {best_result[1]:.4f} ({best_result[0]})")
            print(f"   ì›ë³¸ ëŒ€ë¹„: {best_result[1] - original_fid:+.4f}")
            print(f"   ëª©í‘œ(20)ê¹Œì§€: {best_result[1] - 20:.4f} ë‚¨ìŒ")
        else:
            print(f"âš ï¸ ê°œì„  ì‹¤íŒ¨. ì›ë³¸ì´ ìµœì„ : {original_fid:.4f}")
    
    return results


def analyze_covariance_contribution(features_A: np.ndarray, features_B: np.ndarray):
    """ê³µë¶„ì‚° termì˜ ìƒì„¸ ë¶„ì„"""
    
    print("\n" + "=" * 60)
    print("ğŸ“ ê³µë¶„ì‚° ìƒì„¸ ë¶„ì„")
    print("=" * 60)
    
    sigma_A = np.cov(features_A, rowvar=False)
    sigma_B = np.cov(features_B, rowvar=False)
    
    eigvals_A = np.linalg.eigvalsh(sigma_A)
    eigvals_B = np.linalg.eigvalsh(sigma_B)
    
    print(f"\nê³µë¶„ì‚° í–‰ë ¬ ê³ ìœ ê°’ ë¶„ì„:")
    print(f"  Real (A) [n={len(features_A)}]:")
    print(f"    - ìµœëŒ€ ê³ ìœ ê°’: {eigvals_A.max():.4f}")
    print(f"    - ìµœì†Œ ê³ ìœ ê°’: {eigvals_A.min():.4f}")
    print(f"    - ì¡°ê±´ìˆ˜: {eigvals_A.max() / (eigvals_A.min() + 1e-10):.2f}")
    print(f"    - Trace: {np.trace(sigma_A):.2f}")
    
    print(f"  Gen (B) [n={len(features_B)}]:")
    print(f"    - ìµœëŒ€ ê³ ìœ ê°’: {eigvals_B.max():.4f}")
    print(f"    - ìµœì†Œ ê³ ìœ ê°’: {eigvals_B.min():.4f}")
    print(f"    - ì¡°ê±´ìˆ˜: {eigvals_B.max() / (eigvals_B.min() + 1e-10):.2f}")
    print(f"    - Trace: {np.trace(sigma_B):.2f}")
    
    cov_diff = sigma_A - sigma_B
    frob_norm = np.linalg.norm(cov_diff, 'fro')
    print(f"\n  ê³µë¶„ì‚° ì°¨ì´ (Frobenius norm): {frob_norm:.2f}")
    
    return sigma_A, sigma_B, eigvals_A, eigvals_B


def main(args):
    device = torch.device(args.device)
    
    print("=" * 80)
    print("ğŸ”¬ ê³µë¶„ì‚° ë¶„ì„ ê¸°ë°˜ FID ê°œì„  ë„êµ¬ (v3)")
    print("   ëª©í‘œ: FID < 20")
    print("=" * 80)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“ Real ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {args.real_dir}")
    print(f"ğŸ“ Gen ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {args.gen_dir}")
    
    real_dataset = ImageDataset([args.real_dir])
    gen_dataset = ImageDataset([args.gen_dir])
    
    print(f"\n  Real ì´ë¯¸ì§€ ìˆ˜: {len(real_dataset)}")
    print(f"  Gen ì´ë¯¸ì§€ ìˆ˜: {len(gen_dataset)}")
    
    real_loader = DataLoader(real_dataset, batch_size=args.batch_size, 
                             shuffle=False, num_workers=args.num_workers)
    gen_loader = DataLoader(gen_dataset, batch_size=args.batch_size, 
                            shuffle=False, num_workers=args.num_workers)
    
    # 2. Feature ì¶”ì¶œ
    print("\n" + "=" * 80)
    print("ğŸ§  InceptionV3 Feature ì¶”ì¶œ")
    print("=" * 80)
    
    model = InceptionV3FeatureExtractor().to(device)
    
    print("\n[Real ì´ë¯¸ì§€ feature ì¶”ì¶œ]")
    real_features = extract_features(real_loader, model, device)
    
    print("\n[Gen ì´ë¯¸ì§€ feature ì¶”ì¶œ]")
    gen_features = extract_features(gen_loader, model, device)
    
    print(f"\n  Real features shape: {real_features.shape}")
    print(f"  Gen features shape: {gen_features.shape}")
    
    # 3. ì›ë³¸ FID ê³„ì‚°
    print("\n" + "=" * 80)
    print("ğŸ“Š ì›ë³¸ FID ê³„ì‚°")
    print("=" * 80)
    
    original_fid, mean_term, cov_term = compute_fid(real_features, gen_features)
    print(f"\n  í‰ê·  term: {mean_term:.4f}")
    print(f"  ê³µë¶„ì‚° term: {cov_term:.4f}")
    print(f"  ì›ë³¸ FID: {original_fid:.4f}")
    print(f"\n  ê³µë¶„ì‚° ê¸°ì—¬ë„: {cov_term/original_fid*100:.1f}%")
    
    # 4. ê³µë¶„ì‚° ìƒì„¸ ë¶„ì„
    analyze_covariance_contribution(real_features, gen_features)
    
    # 5. ëª¨ë“  v5 ì „ëµ í‰ê°€ (ëª©í‘œ: FID < 20)
    results = evaluate_all_strategies_v4(real_features, gen_features, device=args.device)
    
    # 6. Feature ë° ê²°ê³¼ ì €ì¥
    # ìµœê³  ê²°ê³¼ ì°¾ê¸°
    best_result = None
    for r in results:
        if r[0] != "ì›ë³¸ (B ì „ì²´)" and r[5] is not None:
            if best_result is None or r[1] < best_result[1]:
                best_result = r
    
    if args.save_features:
        save_path = os.path.join(os.path.dirname(args.gen_dir), "fid_optimization_results.npz")
        save_dict = {
            'real_features': real_features,
            'gen_features': gen_features,
        }
        if best_result:
            save_dict['best_indices'] = best_result[5]
            save_dict['best_fid'] = best_result[1]
            save_dict['best_strategy'] = best_result[0]
        
        np.savez(save_path, **save_dict)
        print(f"\n  ê²°ê³¼ ì €ì¥ë¨: {save_path}")
    
    # 7. ìµœì  FID ì´ë¯¸ì§€ ê²½ë¡œë“¤ì„ JSONìœ¼ë¡œ ì €ì¥
    if args.save_json and best_result is not None:
        best_indices = best_result[5]
        best_fid = best_result[1]
        best_strategy = best_result[0]
        
        # gen_datasetì˜ ì´ë¯¸ì§€ ê²½ë¡œ ëª©ë¡ì—ì„œ ìµœì  ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ê²½ë¡œ ì¶”ì¶œ
        selected_paths = [gen_dataset.image_paths[i] for i in best_indices]
        
        # JSONìœ¼ë¡œ ì €ì¥í•  ë‚´ìš© êµ¬ì„±
        json_output = {
            "strategy": best_strategy,
            "fid": float(best_fid),
            "mean_term": float(best_result[2]),
            "cov_term": float(best_result[3]),
            "total_gen_images": len(gen_dataset),
            "selected_count": len(selected_paths),
            "selected_ratio": len(selected_paths) / len(gen_dataset),
            "real_dir": args.real_dir,
            "gen_dir": args.gen_dir,
            "selected_paths": selected_paths
        }
        
        # JSON íŒŒì¼ ì €ì¥
        if args.json_output:
            json_save_path = args.json_output
        else:
            json_save_path = os.path.join(os.path.dirname(args.gen_dir), "best_fid_selected_paths.json")
        
        with open(json_save_path, 'w', encoding='utf-8') as f:
            json.dump(json_output, f, ensure_ascii=False, indent=2)
        
        print(f"\n" + "=" * 80)
        print(f"ğŸ“„ ìµœì  FID ì´ë¯¸ì§€ ê²½ë¡œ JSON ì €ì¥")
        print(f"=" * 80)
        print(f"  ì „ëµ: {best_strategy}")
        print(f"  FID: {best_fid:.4f}")
        print(f"  ì„ íƒëœ ì´ë¯¸ì§€: {len(selected_paths)}ê°œ / {len(gen_dataset)}ê°œ ({len(selected_paths)/len(gen_dataset)*100:.1f}%)")
        print(f"  ì €ì¥ ê²½ë¡œ: {json_save_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ê³µë¶„ì‚° ë¶„ì„ ê¸°ë°˜ FID ê°œì„  ë„êµ¬ v3")
    parser.add_argument("--real_dir", type=str, required=True, 
                        help="Real ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬")
    parser.add_argument("--gen_dir", type=str, required=True, 
                        help="Generated ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬")
    parser.add_argument("--batch_size", type=int, default=32, 
                        help="ë°°ì¹˜ ì‚¬ì´ì¦ˆ (default: 32)")
    parser.add_argument("--n_clusters", type=int, default=50, 
                        help="K-Means í´ëŸ¬ìŠ¤í„° ìˆ˜ (default: 50)")
    parser.add_argument("--num_workers", type=int, default=8, 
                        help="DataLoader worker ìˆ˜ (default: 8)")
    parser.add_argument("--device", type=str, 
                        default="cuda" if torch.cuda.is_available() else "cpu",
                        help="ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (default: cuda)")
    parser.add_argument("--save_features", action="store_true",
                        help="ì¶”ì¶œëœ features ë° ê²°ê³¼ ì €ì¥ ì—¬ë¶€")
    parser.add_argument("--save_json", action="store_true",
                        help="ìµœì  FIDë¥¼ ë‹¬ì„±í•œ ì´ë¯¸ì§€ ê²½ë¡œë“¤ì„ JSONìœ¼ë¡œ ì €ì¥")
    parser.add_argument("--json_output", type=str, default=None,
                        help="JSON ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ gen_dir ë¶€ëª¨ í´ë”ì— ì €ì¥)")
    
    args = parser.parse_args()
    main(args)
